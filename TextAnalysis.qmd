---
title: "Stock Sentiment Analysis"
author: "Matthew Clarke"
format: pdf
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

The `echo: false` option disables the printing of code (only output is displayed).

## Execute Code - WordCloud

Code to pull an Earnings Report from Q4 2025 NVDA to create a Wordcloud

```{r}
#install.packages("tm")        #Text mining
#install.packages("tidytext")  #Tidytext principles
#install.packages("dplyr")     #Data manipulation
#install.packages("stringr")   #String operations
#install.packages("wordcloud") #Word clouds
#install.packages("textdata")  #Sentiment lexicons
install.packages("tidyr")     #Net Sentiment
library(tm)
library(tidytext)
library(dplyr)
library(stringr)
library(wordcloud)
library(textdata)
library(tidyr)

#Attempting to scrape Earnings Calls from Montley Fool

library(rvest)
library(dplyr)
library(stringr)

# Define the URL (Motley Fool transcript)
url1 <- "https://www.fool.com/earnings/call-transcripts/2024/05/29/nvidia-nvda-q1-2025-earnings-call-transcript/"
url2 <- "https://www.fool.com/earnings/call-transcripts/2024/08/28/nvidia-nvda-q2-2025-earnings-call-transcript/"
url3 <- "https://www.fool.com/earnings/call-transcripts/2024/11/20/nvidia-nvda-q3-2025-earnings-call-transcript/"
url4 <- "https://www.fool.com/earnings/call-transcripts/2025/02/26/nvidia-nvda-q4-2025-earnings-call-transcript/"

# Read the page
page1 <- read_html(url1)
page2 <- read_html(url2)
page3 <- read_html(url3)
page4 <- read_html(url4)

# Inspect the page to find the correct node. Typically, transcripts are within article divs:
transcript_text <- page %>%
  html_nodes("article") %>%        # or another relevant selector such as "div.article-content"
  html_text(trim = TRUE)

#Traditional Text Upload Method

Q42025_NVDA <- readLines("C:/Users/mattc/git-folder/Stock-Sentiment/NVDA_Q4_2025_Earnings Call.txt")

#Using TM
#Clean_Data <- Corpus(VectorSource(Q42025_NVDA)) 
Clean_Data <- VCorpus(VectorSource(Q42025_NVDA))

# Convert to lowercase
Cleaning <- tm_map(Clean_Data, content_transformer(tolower))
# Remove punctuation
Cleaning <- tm_map(Cleaning, removePunctuation)
# Remove numbers
Cleaning <- tm_map(Cleaning, removeNumbers)
# Remove stopwords (common words like "the", "is")
Cleaning <- tm_map(Cleaning, removeWords, stopwords("english"))
# Strip whitespace
Cleaning <- tm_map(Cleaning, stripWhitespace)

#Building DTM
dtm <- DocumentTermMatrix(Cleaning)
# Convert DTM to a matrix
m <- as.matrix(dtm)

# Word frequencies
word_freq <- sort(colSums(m), decreasing = TRUE)

# Top 10 most frequent words
head(word_freq, 10)

#Visualization
set.seed(1234)
wordcloud(names(word_freq), word_freq, min.freq = 2, max.words = 100, random.order = FALSE)

```

Code to figure out positive and negative word sentiment counts

```{r}

file <- tibble(text = Q42025_NVDA)
tokens <- file %>%
  unnest_tokens(word, text)

#Remove stopwords and count
tokens %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)

#Sentiment Frequency

bing <- get_sentiments("bing")  # Bing lexicon

tokens %>%
  inner_join(bing, by = "word") %>%
  count(sentiment)

sentiment <- inner_join(tokens, bing, by = "word")

positive = 0
negative = 0
for(i in 1:length(sentiment$sentiment))
{
  if(sentiment$sentiment[i] == "positive")
  {
    positive = positive + 1
  }
  if(sentiment$sentiment[i] == "negative")
  {
    negative = negative + 1
  }
}
tibble("Q42025", positive, negative)

#Net Sentiment Score
positive-negative
NetSentiment = rbind(positive, negative)

#Visualization
barplot(NetSentiment, beside = TRUE, col = c("green","red"), names.arg = c("Positive", "Negative"),main = "Sentiment in Earning Call", ylab = "Count of Words")
```
